{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset to take a quick look at the first few rows\n",
    "file_path = './train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Displaying the first few rows of the dataset to get an overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Razonamineto Preprocesamiento\n",
    "\n",
    "### 1. Convertir el texto a minúsculas:\n",
    "**Razón:**  Esto se hace para garantizar que la misma palabra con diferentes casos no se trate como palabras distintas. Por ejemplo, \"Hola\" y \"hola\" se tratarían como la misma palabra.\n",
    "### 2. Quitar URLs:\n",
    "**Razón:** Las URLs generalmente no añaden información significativa para el análisis de texto y pueden ser una fuente de ruido, especialmente en tareas como la clasificación de texto.\n",
    "### 3. Quitar caracteres especiales como #, @, y apóstrofes:\n",
    "**Razón:** Estos caracteres suelen ser ruidosos y no aportan mucho a la semántica del texto. Sin embargo, en algunos casos, como el análisis de sentimientos en tweets, podrían ser útiles.\n",
    "### 4. Quitar emoticones:\n",
    "**Razón:** Los emoticones pueden ser útiles para algunas tareas específicas como el análisis de sentimientos, pero en general pueden considerarse como ruido en el texto.\n",
    "### 5. Quitar signos de puntuación:\n",
    "**Razón:**  Los signos de puntuación raramente aportan valor en tareas de análisis de texto y generalmente se consideran ruido.\n",
    "### 6. Quitar palabras vacías (stopwords):\n",
    "**Razón:** Palabras como \"y\", \"o\", \"el\", \"la\", etc., son muy comunes pero no aportan información significativa para muchas tareas de análisis de texto.\n",
    "### 7. Quitar números:\n",
    "**Razón:**  Los números pueden ser útiles para ciertas tareas, pero en muchos casos son irrelevantes. Por ejemplo, en el análisis de sentimientos, los números raramente aportan algún sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deeds reason may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>notified officers evacuation shelter place ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive evacuation orders california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0                  deeds reason may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  notified officers evacuation shelter place ord...  \n",
       "3        people receive evacuation orders california  \n",
       "4             got sent photo ruby smoke pours school  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries for text preprocessing\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters like #, @, and apostrophes\n",
    "    text = re.sub(r'\\@\\w+|\\#\\w+|\\'[\\w\\d\\s]+|[\\w\\d\\s]+\\'', '', text)\n",
    "    \n",
    "    # Remove emoticons\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Remove numbers (we can customize this further based on domain-specific needs)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Applying the preprocessing function to the 'text' column\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Displaying the first few rows to see the changes\n",
    "df[['text', 'preprocessed_text']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7613, 23]), 12789)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenized_text = [text.split() for text in df['preprocessed_text']]\n",
    "\n",
    "# Count the occurrences of each word to create a vocabulary\n",
    "flat_list = [word for sublist in tokenized_text for word in sublist]\n",
    "word_counter = Counter(flat_list)\n",
    "vocab = {word: i+1 for i, (word, _) in enumerate(word_counter.most_common())}\n",
    "\n",
    "# Convert the tokenized text to numerical format based on the vocabulary\n",
    "numericalized_text = [[vocab[word] for word in text] for text in tokenized_text]\n",
    "\n",
    "# Padding: Make all sequences have the same length\n",
    "# First, find the sequence with maximum length\n",
    "max_len = max(len(seq) for seq in numericalized_text)\n",
    "\n",
    "# Pad sequences with zeros at the end\n",
    "padded_sequences = [seq + [0]*(max_len - len(seq)) for seq in numericalized_text]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tensor_sequences = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "tensor_labels = torch.tensor(df['target'].values, dtype=torch.float32)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tensor_sequences, tensor_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=64)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=64)\n",
    "\n",
    "vocab_size = len(vocab) + 1  # Adding 1 for padding (0)\n",
    "tensor_sequences.shape, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(TweetClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Sigmoid Activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Parámetros del modelo\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "\n",
    "# Instancia del modelo\n",
    "model = TweetClassifier(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Función de Pérdida y Optimizador\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.6837, test loss 0.6827\n",
      "Epoch 10: train loss 0.6834, test loss 0.6822\n",
      "Epoch 20: train loss 0.0611, test loss 1.0010\n",
      "Epoch 30: train loss 0.0379, test loss 1.5286\n",
      "Epoch 40: train loss 0.0343, test loss 1.9039\n",
      "Epoch 50: train loss 0.0456, test loss 1.5087\n",
      "Epoch 60: train loss 0.0336, test loss 1.8403\n",
      "Epoch 70: train loss 0.0335, test loss 1.9411\n",
      "Epoch 80: train loss 0.0335, test loss 1.9126\n",
      "Epoch 90: train loss 0.0334, test loss 2.0605\n",
      "Epoch 99: train loss 0.0360, test loss 1.3992\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Asumiendo que 'TweetClassifier' es tu modelo LSTM personalizado para este problema\n",
    "\n",
    "# Inicialización del modelo, optimizador y función de pérdida\n",
    "model = TweetClassifier(vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# DataLoader\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=64)\n",
    "\n",
    "# Número de épocas\n",
    "n_epochs = 100\n",
    "\n",
    "# Listas para almacenar las métricas\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validación cada 100 épocas\n",
    "    if epoch % 10 != 0 and epoch != n_epochs - 1:\n",
    "        continue\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Cálculo de métricas para el conjunto de entrenamiento\n",
    "        y_pred_train = model(X_train).squeeze()\n",
    "        train_loss = loss_fn(y_pred_train, y_train)\n",
    "        \n",
    "        # Cálculo de métricas para el conjunto de prueba\n",
    "        y_pred_test = model(X_test).squeeze()\n",
    "        test_loss = loss_fn(y_pred_test, y_test)\n",
    "        \n",
    "        # Aquí puedes añadir el cálculo de otras métricas como precisión, recall, etc.\n",
    "        \n",
    "    print(f\"Epoch {epoch}: train loss {train_loss:.4f}, test loss {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar las etiquetas verdaderas\n",
    "test_labels = []\n",
    "\n",
    "# No es necesario calcular gradientes\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Obtener datos y etiquetas del batch\n",
    "        _, batch_labels = batch\n",
    "        \n",
    "        # Almacenar las etiquetas verdaderas\n",
    "        test_labels.extend(batch_labels.tolist())\n",
    "\n",
    "# Convertir las etiquetas verdaderas a una lista de enteros (0 o 1)\n",
    "test_labels = [int(label) for label in test_labels]\n",
    "\n",
    "# Cambiar el modelo a modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Lista para almacenar las predicciones\n",
    "test_preds = []\n",
    "\n",
    "# No calcular gradientes para acelerar la computación\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Obtener datos y etiquetas del batch\n",
    "        batch_data, _ = batch\n",
    "        \n",
    "        # Hacer predicciones\n",
    "        predictions = model(batch_data).squeeze()\n",
    "        \n",
    "        # Almacenar las predicciones\n",
    "        test_preds.extend(predictions.tolist())\n",
    "\n",
    "# Convertir las predicciones a etiquetas binarias (0 o 1)\n",
    "rounded_preds = [round(pred) for pred in test_preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7360472751149048\n",
      "Precision: 0.6963434022257552\n",
      "Recall: 0.674884437596302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Redondear las predicciones para obtener etiquetas binarias\n",
    "rounded_preds = [round(pred) for pred in test_preds]\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(test_labels, rounded_preds)\n",
    "precision = precision_score(test_labels, rounded_preds)\n",
    "recall = recall_score(test_labels, rounded_preds)\n",
    "f1 = f1_score(test_labels, rounded_preds)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(new_tweet, model, vocab, max_len):\n",
    "    # Preprocesar el tweet\n",
    "    processed_tweet = preprocess_text(new_tweet)\n",
    "    \n",
    "    # Tokenizar y aplicar padding\n",
    "    tokenized = [vocab[word] for word in processed_tweet.split() if word in vocab]\n",
    "    padded = tokenized + [0] * (max_len - len(tokenized))\n",
    "    \n",
    "    # Convertir a tensor y pasar por el modelo\n",
    "    tweet_tensor = torch.tensor([padded], dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tweet_tensor).item()\n",
    "    \n",
    "    # Interpretar la salida del modelo\n",
    "    if prediction >= 0.5:\n",
    "        return \"\\33[91m Este tweet probablemente se trata de un desastre natural real. \\33[0m\"\n",
    "    else:\n",
    "        return \"\\33[92m Este tweet probablemente no se trata de un desastre natural real. \\33[0m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Este tweet probablemente no se trata de un desastre natural real. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Leer input del usuario\n",
    "new_tweet = input(\"Escribe un tweet: \")\n",
    "result = classify_tweet(new_tweet, model, vocab, max_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion del algormio utilizado: \n",
    "## 1. Preprocesamiento de Datos\n",
    "Convertimos todo el texto a minúsculas para unificar el formato.\n",
    "Eliminamos URLs, caracteres especiales, emoticones y signos de puntuación para reducir el ruido.\n",
    "Eliminamos palabras vacías (stopwords) y números para centrarnos en las palabras más significativas.\n",
    "## 2. Tokenización y Padding\n",
    "Dividimos el texto en palabras (tokens) y creamos un vocabulario que asigna un número único a cada palabra.\n",
    "Convertimos las palabras en cada tweet a su número correspondiente según el vocabulario.\n",
    "Rellenamos las secuencias con ceros para que todas tengan la misma longitud.\n",
    "## 3. División del Conjunto de Datos\n",
    "Dividimos el conjunto de datos en conjuntos de entrenamiento y prueba.\n",
    "## 4. Preparación del DataLoader\n",
    "Utilizamos el DataLoader de PyTorch para manejar el muestreo de batches durante el entrenamiento.\n",
    "## 5. Construcción del Modelo\n",
    "Creamos un modelo de red neuronal utilizando PyTorch, que incluye una capa de embedding, una capa LSTM y una capa totalmente conectada.\n",
    "## 6. Compilación del Modelo\n",
    "Utilizamos el optimizador Adam y la función de pérdida de entropía cruzada binaria (BCELoss).\n",
    "## 7. Entrenamiento del Modelo\n",
    "Entrenamos el modelo a través de múltiples épocas, utilizando backpropagation y optimización de Adam.\n",
    "En cada época, calculamos y almacenamos la pérdida en los conjuntos de entrenamiento y prueba.\n",
    "## 8. Evaluación del Modelo\n",
    "Utilizamos el modelo entrenado para hacer predicciones en el conjunto de prueba.\n",
    "Calculamos métricas como precisión, recall y F1-score para evaluar el rendimiento del modelo.\n",
    "Este algoritmo proporciona un enfoque sólido para clasificar tweets en categorías relacionadas con desastres naturales. El uso de una arquitectura LSTM permite al modelo capturar dependencias temporales en los datos de texto, lo que es crucial para entender el contexto y hacer predicciones más precisas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
